{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sketchRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ij264/Corpus-Drawing-Project/blob/master/sketchRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AhiOqKusgTn",
        "outputId": "39063e90-9cc1-477a-8e05-dcea0ee2dc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime\n",
        "\n",
        "# imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import random\n",
        "\n",
        "# hyperparameters\n",
        "# UPDATE\n",
        "\n",
        "hp = {\n",
        "    'location': '/content/drive/Shared drives/Corpus Drawing Project/data/sketchrnn_airplane.npz',\n",
        "    'Nz': 128,\n",
        "    'batch_size': 25,\n",
        "    'encoder_hidden_size': 256,\n",
        "    'decoder_hidden_size': 512,\n",
        "    'temperature': 0.1,\n",
        "    'gradient_clipping': 1.0,\n",
        "    'lr': 1e-3,\n",
        "    'min_lr' : 1e-5,\n",
        "    'decay_rate': 0.99999,\n",
        "    'KL_min': 0.2,\n",
        "    'eta_min': 1e-2,\n",
        "    'R': 0.99995,\n",
        "    'M': 20,\n",
        "    'wKL': 0.5,\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "# returns maximum sequence length in stroke sequences in data\n",
        "def get_max_length(data):\n",
        "    sequences = [len(seq) for seq in data]\n",
        "    return max(sequences)\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self,\n",
        "                 strokes,\n",
        "                 batch_size=hp['batch_size'],\n",
        "                 random_scale_factor=0.0,\n",
        "                 augment_stroke_prob=0.0,\n",
        "                 limit=1000):\n",
        "        self.batch_size = batch_size # Batch size.\n",
        "        self.max_seq_length = get_max_length(strokes) # Nmax.\n",
        "        self.random_scale_factor = random_scale_factor # Data augmentation method.\n",
        "\n",
        "        # Removes large gaps in data. x and y offets are clamped to have absolute values no greater than this limit.\n",
        "        self.limit = limit\n",
        "        self.augment_stroke_prob = augment_stroke_prob\n",
        "\n",
        "        # sets self.strokes: list of arrays (sorted by size), one per sketch, in stroke-3 format (DeltaX, DeltaY, pen binary state)\n",
        "        self.preprocess(strokes)\n",
        "        self.pad_data(self.strokes, self.max_seq_length)\n",
        "        self.normalise()\n",
        "    \n",
        "    def preprocess(self, strokes): \n",
        "        # Removes entries from strokes having a sequence longer than max_seq_lengths\n",
        "        raw_data = []\n",
        "        seq_len = []\n",
        "        count_data = 0\n",
        "        \n",
        "        for data in strokes:\n",
        "\n",
        "            if len(data) <= (self.max_seq_length):\n",
        "                count_data += 1\n",
        "                # removes large gaps from the data\n",
        "                data = np.minimum(data, self.limit)\n",
        "                data = np.maximum(data, -self.limit)\n",
        "                raw_data.append(data)\n",
        "                seq_len.append(len(data))\n",
        "\n",
        "        seq_len = np.array(seq_len)  \n",
        "        idx = np.argsort(seq_len)\n",
        "\n",
        "        self.strokes = []\n",
        "\n",
        "        for i in range(len(seq_len)):\n",
        "            self.strokes.append(raw_data[idx[i]])\n",
        "\n",
        "        print(\"total images <= max_seq_len is %d\" % count_data)\n",
        "\n",
        "        self.num_batches = int(count_data / self.batch_size)\n",
        "        return self.strokes\n",
        "\n",
        "    def calculate_normalizing_scale_factor(self):\n",
        "        ''' Calculate the normalizing factor explained in appendix of sketch-rnn '''\n",
        "        return torch.std(self.strokes) \n",
        "\n",
        "    def normalise(self):\n",
        "        ''' Normalise entire dataset by normalising factor '''\n",
        "        scale_factor = self.calculate_normalizing_scale_factor()\n",
        "        self.strokes[:,:,0:2] /= scale_factor\n",
        "        return self.strokes\n",
        "\n",
        "    def pad_data(self, data, max_len):\n",
        "        ''' Pad the batch to be stroke-5 bigger format as described in paper '''\n",
        "        padded_data = np.zeros((len(data), max_len, 5), dtype=float)\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            l = len(data[i])\n",
        "            assert l <= max_len\n",
        "            padded_data[i, 0:l, 0:2] = data[i][:, 0:2]\n",
        "            padded_data[i, 0:l, 3] = data[i][:, 2]\n",
        "            padded_data[i, 0:l, 2] = 1 - padded_data[i, 0:l, 3]\n",
        "            padded_data[i, l:, 4] = 1\n",
        "            padded_data[i, 1:, :] = padded_data[i, :-1, :]\n",
        "            padded_data[i, 0, :] = 0 # All zeros  but should change it to be S_0 = torch.tensor([0, 0, 1, 0, 0]) since that SHOULD be the initial stroke on each of the drawings. However, removing this line entirely seems to change the stroke values.\n",
        "\n",
        "        self.strokes = torch.from_numpy(padded_data)\n",
        "        return self.strokes # Returns a tensor in the 5-vector format described in the paper.\n",
        "\n",
        "data = np.load(hp['location'], encoding='latin1', allow_pickle=True)\n",
        "train_strokes = data['train'][:100]\n",
        "test_strokes = data['test']\n",
        "train_set = DataLoader(train_strokes).strokes\n",
        "print(train_set)\n",
        "Nmax = get_max_length(train_strokes) #TK Does it matter we're using train strokes instead of trainset?\n",
        "print(Nmax)\n",
        "# preprocessing done!!!!\n",
        "def lr_decay(optimiser):\n",
        "    for param_group in optimiser.param_groups:\n",
        "        if param_group['lr'] > hp['min_lr']:\n",
        "            param_group['lr'] *= hp['decay_rate']\n",
        "    return optimiser\n",
        "\n",
        "# encoder RNN\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # bidirectional LSTM \n",
        "        self.LSTM = nn.LSTM(5, \n",
        "                            hp['encoder_hidden_size'],\n",
        "                            bidirectional=True) \n",
        "        self.dropout = nn.Dropout(hp['dropout'])\n",
        "        # mu and sigma from LSTM's output\n",
        "        self.fc_mu = nn.Linear(2*hp['encoder_hidden_size'],\n",
        "                               hp['Nz'])\n",
        "        self.fc_sigma = nn.Linear(2*hp['encoder_hidden_size'], \n",
        "                                  hp['Nz']) #2x since we append the h backward and forward processes together for a bidirectional LSTM\n",
        "        \n",
        "        self.train()\n",
        "\n",
        "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
        "        if hidden_cell is None:\n",
        "            # initialise with zeros\n",
        "            hidden = torch.zeros(2, batch_size, hp['encoder_hidden_size'])\n",
        "            cell = torch.zeros(2, batch_size, hp['encoder_hidden_size'])\n",
        "            hidden_cell = (hidden, cell)\n",
        "\n",
        "        _, (hidden, cell) = self.LSTM(inputs.float(), hidden_cell) # returns hidden state and cell vector. we discard the output tensor\n",
        "        hidden_forward, hidden_backward = torch.split(self.dropout(hidden), 1, 0) # returns forward and backwards\n",
        "        hidden_concat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)], 1) # concatenates the forward and backwards h \n",
        "\n",
        "        mu = self.fc_mu(hidden_concat)\n",
        "        sigma_hat = self.fc_sigma(hidden_concat)\n",
        "        sigma = torch.exp(sigma_hat/2.)\n",
        "\n",
        "        N = torch.normal(torch.zeros(mu.size()), torch.ones(mu.size())) \n",
        "\n",
        "        z = mu + N * sigma\n",
        "        print('z size:', z.size())\n",
        "        return z, mu, sigma_hat\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.fc_hc = nn.Linear(hp['Nz'],\n",
        "                               2 * hp['decoder_hidden_size']) #Are we potentially missing the tanh part of this layer as well? Not sure where it should go but in the paper it seems to suggest there should be a tanh layer on top of this FC linear layer\n",
        "        \n",
        "        # unidirectional LSTM \n",
        "        self.LSTM = nn.LSTM(hp['Nz'] + 5, # input of decoder is output of encoder (latent vector of size Nz) as well as the previous data point, S_{i-1}\n",
        "                            hp['decoder_hidden_size'])\n",
        "        \n",
        "        self.dropout = nn.Dropout(hp['dropout'])\n",
        "        \n",
        "        self.fc_params = nn.Linear(hp['decoder_hidden_size'], \n",
        "                                   6 * hp['M'] + 3)\n",
        "        \n",
        "    def forward(self, inputs, z, hidden_cell=None):\n",
        "        if hidden_cell is None:\n",
        "            hidden, cell = torch.split(torch.tanh(self.fc_hc(z)), hp['decoder_hidden_size'], 1)\n",
        "            hidden_cell = (hidden.unsqueeze(0).contiguous(), cell.unsqueeze(0).contiguous())\n",
        "\n",
        "        outputs, (hidden, cell) = self.LSTM(inputs, hidden_cell)\n",
        "\n",
        "        if self.training:\n",
        "            y = self.fc_params(outputs.view(-1, hp['decoder_hidden_size']))\n",
        "\n",
        "        else:\n",
        "            y = self.fc_params(self.dropout(hidden).view(-1, hp['decoder_hidden_size']))\n",
        "\n",
        "        params = torch.split(y, 6, 1)\n",
        "        mixture_params = torch.stack(params[:-1])\n",
        "\n",
        "        q_hat = params[-1]\n",
        "        Pi_hat, mu_x, mu_y, sigma_x_hat, sigma_y_hat, rho_xy_hat = torch.split(mixture_params, 1, 2)\n",
        "\n",
        "        if self.training:\n",
        "            len_out = Nmax + 1\n",
        "        else:\n",
        "            len_out = 1\n",
        "\n",
        "        Pi = F.softmax(Pi_hat.transpose(0, 1).squeeze()).view(len_out, -1, hp['M'])\n",
        "        sigma_x = torch.exp(sigma_x_hat.transpose(0, 1).squeeze()).view(len_out, -1, hp['M'])\n",
        "        sigma_y = torch.exp(sigma_y_hat.transpose(0, 1).squeeze()).view(len_out, -1, hp['M'])\n",
        "        rho_xy = torch.tanh(rho_xy_hat.transpose(0, 1).squeeze()).view(len_out, -1, hp['M'])\n",
        "        mu_x = mu_x.transpose(0, 1).squeeze().contiguous().view(len_out, -1, hp['M'])\n",
        "        mu_y = mu_y.transpose(0, 1).squeeze().contiguous().view(len_out, -1, hp['M'])\n",
        "        q = F.softmax(q_hat, dim=1).view(len_out, -1, 3)\n",
        "\n",
        "        return Pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, hidden, cell\n",
        "\n",
        "batches = list(torch.utils.data.DataLoader(train_set, batch_size=hp['batch_size'], shuffle = True))\n",
        "\n",
        "# NN model: a bidirectional NN with LSTM\n",
        "class Model():\n",
        "    def __init__(self):\n",
        "\n",
        "        # forward encoder\n",
        "        self.encoder = EncoderRNN()\n",
        "\n",
        "        # backward encoder\n",
        "        self.decoder = DecoderRNN()\n",
        "\n",
        "        self.encoder_optimiser = optim.Adam(self.encoder.parameters(), hp['lr'])\n",
        "        self.decoder_optimiser = optim.Adam(self.decoder.parameters(), hp['lr'])\n",
        "        self.eta_step = hp['eta_min']\n",
        "\n",
        "    def make_target(self, batch):\n",
        "        EOS = torch.stack([torch.Tensor([0, 0, 0, 0, 1])] * batch.size()[1]).unsqueeze(0)\n",
        "        batch = torch.cat([batch, EOS], 0)\n",
        "        DX = torch.stack([batch[:, :, 0]] * hp['M'], 2)\n",
        "        DY = torch.stack([batch[:, :, 1]] * hp['M'], 2)\n",
        "        p1 = batch[:, :, 2]\n",
        "        p2 = batch[:, :, 3]\n",
        "        p3 = batch[:, :, 4]\n",
        "\n",
        "        p = torch.stack([p1, p2, p3], 2)\n",
        "\n",
        "        return DX, DY, p\n",
        "\n",
        "    # bivariate normal distribution probability distribution function\n",
        "    def bivariate_normal_PDF(self, Dx, Dy, mu_x, mu_y, sigma_x, sigma_y, rho_xy):\n",
        "\n",
        "        z = (Dx - mu_x)**2/sigma_x**2 \\\n",
        "        - 2 * rho_xy * (Dx - mu_x) * (Dy - mu_y)/(sigma_x * sigma_y) \\\n",
        "        + (Dy - mu_y)**2/sigma_y**2\n",
        "        prefactor = 1/(2 * np.pi * sigma_x * sigma_y * torch.sqrt(1 - rho_xy**2))\n",
        "\n",
        "        return prefactor * torch.exp(- z/(2 * (1 - rho_xy**2)))\n",
        "    '''\n",
        "         mu_x=0\n",
        "         mu_y=0\n",
        "         sigma_x=1\n",
        "         sigma_y=1\n",
        "        M=torch.distributions.multivariate_normal.MultivariateNormal(torch.tensor([mu_x,mu_y]).float(), covariance_matrix = torch.tensor(([sigma_x,0],[0,sigma_y])).float())\n",
        "        torch.exp(M.log_prob(torch.tensor([0,0]))).item()\n",
        "    '''\n",
        "    # reconstruction loss\n",
        "    def LR(self, Dx, Dy, p, epoch):\n",
        "        PDF = self.bivariate_normal_PDF(Dx, Dy, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, self.rho_xy) \n",
        "        # PDF = torch.exp(M.log_prob(torch.tensor([Dx,Dy]))).item() #If you have any problems, might be the .item() on the end of this line. what is M?\n",
        "        LS = - torch.sum(torch.log(1e-5 + torch.sum(self.Pi * PDF, 2)))/float(Nmax) # 1e-5 to prevent log(0)\n",
        "        LP = - torch.sum(p * torch.log(self.q))/float(Nmax)\n",
        "        return LS + LP\n",
        "\n",
        "    # KL divergence loss \n",
        "    # use pytorch function for this \n",
        "    def KL(self):\n",
        "        LKL = -1/(2 * float(hp['Nz'])) * torch.sum(1 + self.sigma_hat - self.mu**2 - torch.exp(self.sigma_hat))\n",
        "        KL_min = torch.Tensor([hp['KL_min']])\n",
        "        return hp['wKL'] * self.eta_step * torch.max(LKL, KL_min)\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "\n",
        "        for batch in batches:\n",
        "            batch = batch.transpose(0, 1)\n",
        "            z, self.mu, self.sigma_hat = self.encoder(batch, hp['batch_size']) # This line does not like sizes. It does now!\n",
        "            sos = torch.stack([torch.Tensor([0, 0, 1, 0, 0])] * hp['batch_size']).unsqueeze(0) # start of sequence\n",
        "            batch_init = torch.cat([sos, batch], 0)\n",
        "            z_stack = torch.stack([z] * (Nmax + 1))\n",
        "            inputs = torch.cat([batch_init, z_stack], 2)\n",
        "            print('batch init:', batch_init.size())\n",
        "            print('z stack:', z_stack.size())\n",
        "            print('inputs:', inputs.size())\n",
        "            self.Pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, self.rho_xy, self.q, _, _ = self.decoder(inputs.float(), z)\n",
        "            DX, DY, p = self.make_target(batch)\n",
        "\n",
        "            self.encoder_optimiser.zero_grad()\n",
        "            self.decoder_optimiser.zero_grad()\n",
        "\n",
        "            self.eta_step = 1 - (1 - hp['eta_min']) * hp['R']\n",
        "\n",
        "            LKL = self.KL()\n",
        "            LR = self.LR(DX, DY, p, epoch)\n",
        "            loss = LR + LKL\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.encoder.parameters(), hp['gradient_clipping'])\n",
        "            nn.utils.clip_grad_norm_(self.decoder.parameters(), hp['gradient_clipping'])\n",
        "\n",
        "            self.encoder_optimiser.step()\n",
        "            self.decoder_optimiser.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print('epoch', epoch,'loss', loss.item(),'LR', LR.item(),'LKL', LKL.item())\n",
        "                self.encoder_optimizer = lr_decay(self.encoder_optimiser)\n",
        "                self.decoder_optimizer = lr_decay(self.decoder_optimiser)\n",
        "            if epoch % 50 == 0:\n",
        "                self.conditional_generation(epoch)\n",
        "      \n",
        "    \"\"\" Samples a sequence from a pre-trained model. \"\"\"\n",
        "    def sample_bivariate_normal(mu_x, mu_y, sigma_x, sigma_y, rho_xy):\n",
        "        mu = [mu_x, mu_y]\n",
        "        sigma_x *= np.sqrt(hp['temperature'])\n",
        "        sigma_y *= np.sqrt(hp['temperature'])\n",
        "        cov = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y], [rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
        "        x = np.random.multivariate_normal(mu, cov, 1)\n",
        "        return x[0][0], x[0][1]\n",
        "\n",
        "    def sample(self):\n",
        "        def adjust_temp(Pi_pdf):\n",
        "            Pi_pdf = torch.tensor(Pi_pdf)\n",
        "            Pi_pdf = torch.log(Pi_pdf) / hp['temperature']\n",
        "            Pi_pdf -= Pi_pdf.max()\n",
        "            Pi_pdf = torch.exp(Pi_pdf)\n",
        "            Pi_pdf /= Pi_pdf.sum()\n",
        "            return Pi_pdf\n",
        "\n",
        "        # get mixture index:\n",
        "        print('size:', self.Pi.size())\n",
        "        Pi = self.Pi.data[0,0,:].numpy()\n",
        "        Pi = adjust_temp(Pi).numpy()\n",
        "        Pi_idx = np.random.choice(hp['M'], p=Pi)\n",
        "        # get pen state:\n",
        "        q = self.q.data[0,0,:].numpy()\n",
        "        q = adjust_temp(q).numpy()\n",
        "        print(q)\n",
        "        q_idx = np.random.choice(3, p=q)\n",
        "        # get mixture params:\n",
        "        mu_x = self.mu_x.data[0, 0, Pi_idx]\n",
        "        mu_y = self.mu_y.data[0, 0, Pi_idx]\n",
        "        sigma_x = self.sigma_x.data[0, 0, Pi_idx]\n",
        "        sigma_y = self.sigma_y.data[0, 0, Pi_idx]\n",
        "        rho_xy = self.rho_xy.data[0, 0, Pi_idx]\n",
        "        x, y = Model.sample_bivariate_normal(mu_x, mu_y, sigma_x, sigma_y, rho_xy)\n",
        "        next_state = torch.zeros(5)\n",
        "        next_state[0] = x\n",
        "        next_state[1] = y\n",
        "        next_state[q_idx + 2] = 1\n",
        "        return next_state.view(1, 1, -1), x, y, q_idx==1, q_idx==2\n",
        "\n",
        "    def make_image(self, sequence, epoch, name='output'):\n",
        "        \"\"\" Plot drawing with separated strokes. \"\"\"\n",
        "        strokes = np.split(sequence, np.where(sequence[:,2]>0)[0] + 1)\n",
        "        fig = plt.figure()\n",
        "        ax1 = fig.add_subplot(111)\n",
        "        for s in strokes:\n",
        "            plt.plot(s[:,0],-s[:,1])\n",
        "        canvas = plt.get_current_fig_manager().canvas\n",
        "        canvas.draw()\n",
        "        pil_image = PIL.Image.frombytes('RGB', canvas.get_width_height(),\n",
        "                    canvas.tostring_rgb())\n",
        "        name = str(epoch)+name+'.jpg'\n",
        "        pil_image.save(name,\"JPEG\")\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    def generate_image(self, Nmax):\n",
        "        for i in range(Nmax): \n",
        "            #input = torch.cat([sos, z.unsqueeze(0)], 2)\n",
        "            # decode \n",
        "            self.Pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, self.rho_xy, self.q, hidden, cell = self.decoder(input.float(), z, hidden_cell)\n",
        "            hidden_cell = (hidden, cell)\n",
        "\n",
        "            S, DeltaX, DeltaY, penState, EOS = self.sample()\n",
        "\n",
        "            seq_DeltaX.append(DeltaX)\n",
        "            seq_DeltaY.append(DeltaY)\n",
        "            seq_penState.append(int(penState))\n",
        "            dummy_penState.append(penState)\n",
        "            if EOS:\n",
        "                print(i)\n",
        "                break\n",
        "        DeltaX_sample = np.cumsum(seq_DeltaX, 0)\n",
        "        DeltaY_sample = np.cumsum(seq_DeltaY, 0)\n",
        "        penState_sample = np.array(seq_penState)\n",
        "        print('dX:', DeltaX_sample)\n",
        "        print('dY:', DeltaY_sample)\n",
        "        print(dummy_penState)\n",
        "        sequence = np.stack([DeltaX_sample, DeltaY_sample, penState_sample]).T\n",
        "        self.make_image(sequence, epoch)\n",
        "\n",
        "    def conditional_generation(self, epoch):\n",
        "        batch = random.choice(batches)\n",
        "        batch = batch.transpose(0, 1)\n",
        "        self.encoder.train(False)\n",
        "        self.decoder.train(False)\n",
        "\n",
        "        z, _, _ = self.encoder(batch, hp['batch_size'])\n",
        "        hidden_cell = None\n",
        "        sos = torch.stack([torch.Tensor([0, 0, 1, 0, 0])] * hp['batch_size']).unsqueeze(0) # start of sequence\n",
        "        seq_DeltaX = []\n",
        "        seq_DeltaY = []\n",
        "        seq_penState = []\n",
        "        dummy_penState = []\n",
        "        batch_init = torch.cat([sos, batch], 0)\n",
        "        z_stack = torch.stack([z] * (Nmax + 1))\n",
        "        input = torch.cat([batch_init, z_stack], 2).float()\n",
        "\n",
        "        for i in range(Nmax): \n",
        "            #input = torch.cat([sos, z.unsqueeze(0)], 2)\n",
        "            # decode \n",
        "            self.Pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, self.rho_xy, self.q, hidden, cell = self.decoder(input.float(), z, hidden_cell)\n",
        "            hidden_cell = (hidden, cell)\n",
        "\n",
        "            S, DeltaX, DeltaY, penState, EOS = self.sample()\n",
        "\n",
        "            seq_DeltaX.append(DeltaX)\n",
        "            seq_DeltaY.append(DeltaY)\n",
        "            seq_penState.append(int(penState))\n",
        "            dummy_penState.append(penState)\n",
        "            if EOS:\n",
        "                print(i)\n",
        "                break\n",
        "        DeltaX_sample = np.cumsum(seq_DeltaX, 0)\n",
        "        DeltaY_sample = np.cumsum(seq_DeltaY, 0)\n",
        "        penState_sample = np.array(seq_penState)\n",
        "        print('dX:', DeltaX_sample)\n",
        "        print('dY:', DeltaY_sample)\n",
        "        print(dummy_penState)\n",
        "        sequence = np.stack([DeltaX_sample, DeltaY_sample, penState_sample]).T\n",
        "        self.make_image(sequence, epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "total images <= max_seq_len is 100\n",
            "tensor([[[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
            "         [ -5.2541,   0.0000,   1.0000,   0.0000,   0.0000],\n",
            "         [ -2.6271,   0.2189,   1.0000,   0.0000,   0.0000],\n",
            "         ...,\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000]],\n",
            "\n",
            "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
            "         [  2.3352,   0.2554,   1.0000,   0.0000,   0.0000],\n",
            "         [  0.3649,   0.1824,   1.0000,   0.0000,   0.0000],\n",
            "         ...,\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000]],\n",
            "\n",
            "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
            "         [  2.0068,   0.0000,   1.0000,   0.0000,   0.0000],\n",
            "         [  0.5473,   0.1095,   1.0000,   0.0000,   0.0000],\n",
            "         ...,\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
            "         [  0.5838,  -1.9703,   1.0000,   0.0000,   0.0000],\n",
            "         [  0.3649,  -0.5838,   1.0000,   0.0000,   0.0000],\n",
            "         ...,\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000]],\n",
            "\n",
            "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
            "         [  2.3717,  -0.0730,   1.0000,   0.0000,   0.0000],\n",
            "         [  2.9555,  -0.3284,   1.0000,   0.0000,   0.0000],\n",
            "         ...,\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000],\n",
            "         [  0.0000,   0.0000,   0.0000,   0.0000,   1.0000]],\n",
            "\n",
            "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
            "         [-14.4124,  -1.4230,   1.0000,   0.0000,   0.0000],\n",
            "         [ -5.2177,  -0.4014,   1.0000,   0.0000,   0.0000],\n",
            "         ...,\n",
            "         [  0.4014,  -0.3284,   1.0000,   0.0000,   0.0000],\n",
            "         [  0.3649,  -0.6568,   1.0000,   0.0000,   0.0000],\n",
            "         [  0.1095,  -0.8027,   1.0000,   0.0000,   0.0000]]],\n",
            "       dtype=torch.float64)\n",
            "99\n",
            "time: 5.16 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjQa7r9iQyjK",
        "outputId": "4d21af5b-482c-4aca-8a5d-879b86ca093b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(train_set[2]) #Make it so it always starts out with 00100\n",
        "print(train_set.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 2.0068,  0.0000,  1.0000,  0.0000,  0.0000],\n",
            "        [ 0.5473,  0.1095,  1.0000,  0.0000,  0.0000],\n",
            "        [ 0.1459,  0.1824,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.0365,  3.9041,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0365,  0.1459,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.0365,  0.1824,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.8757,  0.7297,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0365,  0.0730,  1.0000,  0.0000,  0.0000],\n",
            "        [ 0.9487,  0.0000,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0000,  0.0365,  1.0000,  0.0000,  0.0000],\n",
            "        [ 1.8244,  0.0730,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.3649, -0.3649,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.4743, -0.2189,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0000, -0.0730,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.1459, -1.8244,  1.0000,  0.0000,  0.0000],\n",
            "        [ 0.1095, -2.4446,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0000, -0.1095,  1.0000,  0.0000,  0.0000],\n",
            "        [ 2.9555,  0.0365,  1.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000, -0.1824,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.2919, -0.3649,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.7297, -0.5473,  1.0000,  0.0000,  0.0000],\n",
            "        [-1.7879, -0.7662,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
            "        [ 0.0365, -1.2770,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.1459, -0.5473,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.2919, -0.2919,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.5473, -0.0365,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.1459,  2.2987,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0000,  0.0365,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.9122,  0.0365,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.5473,  0.1824,  1.0000,  0.0000,  0.0000],\n",
            "        [-1.5325,  1.0216,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000]], dtype=torch.float64)\n",
            "torch.Size([100, 99, 5])\n",
            "time: 10.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxJPvakmQyGW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9OrfTKgQWxU",
        "outputId": "05d4d90a-3470-45c0-ca30-6dbc5ca74624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model()\n",
        "for epoch in range(100):\n",
        "    model.train(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:215: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 103.24108123779297 LR 103.24007455090113 LKL 0.0010049500269815326\n",
            "z size: torch.Size([25, 128])\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.24891281 0.2248699  0.5262172 ]\n",
            "0\n",
            "dX: [-0.06754082]\n",
            "dY: [-0.38847025]\n",
            "[False]\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:215: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 102.79297637939453 LR 102.79196554932639 LKL 0.0010049500269815326\n",
            "z size: torch.Size([25, 128])\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.5213245  0.05267164 0.42600384]\n",
            "0\n",
            "dX: [0.18879665]\n",
            "dY: [-0.68326466]\n",
            "[False]\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:215: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 97.74195098876953 LR 97.74094185818686 LKL 0.0010049500269815326\n",
            "z size: torch.Size([25, 128])\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.89800936 0.0022004  0.09979023]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.8980095  0.00220039 0.09979015]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.8980095  0.00220039 0.09979015]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.8980095  0.00220039 0.09979015]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.89800936 0.00220039 0.09979023]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.8980095  0.00220039 0.09979015]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.89800936 0.0022004  0.09979023]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.89800936 0.00220039 0.09979023]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.89800936 0.00220039 0.09979023]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.89800936 0.00220039 0.09979023]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.89800936 0.00220039 0.09979023]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.8980095  0.00220039 0.09979015]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.8980095  0.00220039 0.09979015]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.8980095  0.00220039 0.09979015]\n",
            "13\n",
            "dX: [ 0.36353606  0.33602831 -0.00369049 -0.23657892  0.23708056  0.14347951\n",
            " -0.04007501  0.34401183  0.40945813  0.98871854  1.62354501  1.51086627\n",
            "  1.41024077  1.22138893]\n",
            "dY: [0.13724869 0.17152354 0.8696171  0.443865   0.4444671  0.84430092\n",
            " 1.27055237 0.92589122 0.92453031 1.31543539 0.63779472 0.30599755\n",
            " 0.52913642 0.1802944 ]\n",
            "[False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:215: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 100.18009185791016 LR 100.17908604933359 LKL 0.0010049500269815326\n",
            "z size: torch.Size([25, 128])\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.66036755 0.01977098 0.3198614 ]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.66036785 0.01977097 0.31986123]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.66036785 0.01977097 0.31986123]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.66036755 0.01977098 0.3198614 ]\n",
            "size: torch.Size([1, 25, 20])\n",
            "[0.66036785 0.01977097 0.31986123]\n",
            "4\n",
            "dX: [-0.37525273 -0.56799484 -1.07112702 -0.87866052 -0.76643741]\n",
            "dY: [-0.03990876  0.20998921  0.40578651 -0.19921141  0.05419858]\n",
            "[False, False, False, False, False]\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:215: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n",
            "z size: torch.Size([25, 128])\n",
            "batch init: torch.Size([100, 25, 5])\n",
            "z stack: torch.Size([100, 25, 128])\n",
            "inputs: torch.Size([100, 25, 133])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2c1bedaa454f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-efab90c900c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLKL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_clipping'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "time: 21.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34eb8V03-OVf",
        "cellView": "form"
      },
      "source": [
        "#@title \n",
        "''' Not yet implemented. '''\n",
        "''' Layer norm and hyperLSTM implementation by https://github.com/jihunchoi/hyperlstm/blob/master/models/hyperlstm.py '''\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Implementation of layer normalization, slightly modified from\n",
        "    https://github.com/pytorch/pytorch/issues/1959.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.FloatTensor(num_features))\n",
        "        self.beta = nn.Parameter(torch.FloatTensor(num_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.constant_(self.gamma.data, val=1)\n",
        "        init.constant_(self.beta.data, val=0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        mean = input.mean(dim=-1, keepdim=True)\n",
        "        std = input.std(dim=-1, keepdim=True)\n",
        "        return self.gamma*(input - mean)/(std + self.eps) + self.beta\n",
        "\n",
        "\n",
        "class ParallelLayerNorm(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Faster parallel layer normalization.\n",
        "    Inspired by the implementation of\n",
        "    https://github.com/hardmaru/supercell/blob/master/supercell.py.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, num_features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.FloatTensor(num_inputs, num_features))\n",
        "        self.beta = nn.Parameter(torch.FloatTensor(num_inputs, num_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.constant_(self.gamma.data, val=1)\n",
        "        init.constant_(self.beta.data, val=0)\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_1, ... (Variable): Variables to which\n",
        "                layer normalization be applied. The number of inputs\n",
        "                must be identical to self.num_inputs.\n",
        "        \"\"\"\n",
        "\n",
        "        inputs_stacked = torch.stack(inputs, dim=-2)\n",
        "        mean = inputs_stacked.mean(dim=-1, keepdim=True)\n",
        "        std = inputs_stacked.std(dim=-1, keepdim=True)\n",
        "        outputs_stacked = (self.gamma*(inputs_stacked - mean)/(std + self.eps)\n",
        "                           + self.beta)\n",
        "        outputs = torch.unbind(outputs_stacked, dim=-2)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, use_layer_norm,\n",
        "                 dropout_prob=0):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "        self.linear_ih = nn.Linear(in_features=input_size,\n",
        "                                   out_features=4 * hidden_size)\n",
        "        self.linear_hh = nn.Linear(in_features=hidden_size,\n",
        "                                   out_features=4 * hidden_size,\n",
        "                                   bias=False)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        if use_layer_norm:\n",
        "            self.ln_ifgo = ParallelLayerNorm(num_inputs=4,\n",
        "                                             num_features=hidden_size)\n",
        "            self.ln_c = LayerNorm(hidden_size)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # probs convert to orthognal initialisation\n",
        "        nn.init.xavier_uniform_(self.linear_ih.weight.data)\n",
        "        nn.init.constant_(self.linear_ih.bias.data, val=0)\n",
        "        nn.init.orthogonal_(self.linear_hh.weight.data)\n",
        "        if self.use_layer_norm:\n",
        "            self.ln_ifgo.reset_parameters()\n",
        "            self.ln_c.reset_parameters()\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        if state is None:\n",
        "            batch_size = x.size(0)\n",
        "            zero_state = Variable(\n",
        "                x.data.new(batch_size, self.hidden_size).zero_())\n",
        "            state = (zero_state, zero_state)\n",
        "        h, c = state\n",
        "        lstm_vector = self.linear_ih(x) + self.linear_hh(h)\n",
        "        i, f, g, o = lstm_vector.chunk(chunks=4, dim=1)\n",
        "        if self.use_layer_norm:\n",
        "            i, f, g, o = self.ln_ifgo(i, f, g, o)\n",
        "        f = f + 1\n",
        "        new_c = c*f.sigmoid() + i.sigmoid()*self.dropout(g.tanh())\n",
        "        if self.use_layer_norm:\n",
        "            new_c = self.ln_c(new_c)\n",
        "        new_h = new_c.tanh() * o.sigmoid()\n",
        "        new_state = (new_h, new_c)\n",
        "        return new_h, new_state\n",
        "\n",
        "\n",
        "class HyperLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size,\n",
        "                 hyper_hidden_size, hyper_embedding_size,\n",
        "                 use_layer_norm, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hyper_hidden_size = hyper_hidden_size\n",
        "        self.hyper_embedding_size = hyper_embedding_size\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "        self.hyper_cell = LSTMCell(input_size=input_size + hidden_size,\n",
        "                                   hidden_size=hyper_hidden_size,\n",
        "                                   use_layer_norm=use_layer_norm)\n",
        "        # Hyper LSTM: Projection\n",
        "        for y in ('i', 'f', 'g', 'o'):\n",
        "            proj_h = nn.Linear(in_features=hyper_hidden_size,\n",
        "                               out_features=hyper_embedding_size)\n",
        "            proj_x = nn.Linear(in_features=hyper_hidden_size,\n",
        "                               out_features=hyper_embedding_size)\n",
        "            proj_b = nn.Linear(in_features=hyper_hidden_size,\n",
        "                               out_features=hyper_embedding_size,\n",
        "                               bias=False)\n",
        "            setattr(self, f'hyper_proj_{y}h', proj_h)\n",
        "            setattr(self, f'hyper_proj_{y}x', proj_x)\n",
        "            setattr(self, f'hyper_proj_{y}b', proj_b)\n",
        "        # Hyper LSTM: Scaling\n",
        "        for y in ('i', 'f', 'g', 'o'):\n",
        "            scale_h = nn.Linear(in_features=hyper_embedding_size,\n",
        "                                out_features=hidden_size,\n",
        "                                bias=False)\n",
        "            scale_x = nn.Linear(in_features=hyper_embedding_size,\n",
        "                                out_features=hidden_size,\n",
        "                                bias=False)\n",
        "            scale_b = nn.Linear(in_features=hyper_embedding_size,\n",
        "                                out_features=hidden_size,\n",
        "                                bias=False)\n",
        "            setattr(self, f'hyper_scale_{y}h', scale_h)\n",
        "            setattr(self, f'hyper_scale_{y}x', scale_x)\n",
        "            setattr(self, f'hyper_scale_{y}b', scale_b)\n",
        "        self.linear_ih = nn.Linear(in_features=input_size,\n",
        "                                   out_features=4 * hidden_size,\n",
        "                                   bias=False)\n",
        "        self.linear_hh = nn.Linear(in_features=hidden_size,\n",
        "                                   out_features=4 * hidden_size,\n",
        "                                   bias=False)\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(4 * hidden_size))\n",
        "        if use_layer_norm:\n",
        "            self.ln_ifgo = ParallelLayerNorm(num_inputs=4,\n",
        "                                             num_features=hidden_size)\n",
        "            self.ln_c = LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Hyper LSTM\n",
        "        self.hyper_cell.reset_parameters()\n",
        "        # Hyper LSTM: Projection\n",
        "        for y in ('i', 'g', 'f', 'o'):\n",
        "            proj_h = getattr(self, f'hyper_proj_{y}h')\n",
        "            proj_x = getattr(self, f'hyper_proj_{y}x')\n",
        "            proj_b = getattr(self, f'hyper_proj_{y}b')\n",
        "            nn.init.constant_(proj_h.weight.data, val=0)\n",
        "            nn.init.constant_(proj_h.bias.data, val=1)\n",
        "            nn.init.constant_(proj_x.weight.data, val=0)\n",
        "            nn.init.constant_(proj_x.bias.data, val=1)\n",
        "            nn.init.normal_(proj_b.weight.data, mean=0, std=0.01)\n",
        "        # Hyper LSTM: Scaling\n",
        "        for y in ('i', 'g', 'f', 'o'):\n",
        "            scale_h = getattr(self, f'hyper_scale_{y}h')\n",
        "            scale_x = getattr(self, f'hyper_scale_{y}x')\n",
        "            scale_b = getattr(self, f'hyper_scale_{y}b')\n",
        "            nn.init.constant_(scale_h.weight.data,\n",
        "                          val=0.1 / self.hyper_embedding_size)\n",
        "            nn.init.constant_(scale_x.weight.data,\n",
        "                          val=0.1 / self.hyper_embedding_size)\n",
        "            nn.init.constant_(scale_b.weight.data, val=0)\n",
        "\n",
        "        # Main LSTM\n",
        "        nn.init.xavier_uniform_(self.linear_ih.weight.data)\n",
        "        nn.init.orthogonal_(self.linear_hh.weight.data)\n",
        "        nn.init.constant_(self.bias.data, val=0)\n",
        "\n",
        "        # LayerNorm\n",
        "        if self.use_layer_norm:\n",
        "            self.ln_ifgo.reset_parameters()\n",
        "            self.ln_c.reset_parameters()\n",
        "\n",
        "    def compute_hyper_vector(self, hyper_h, name):\n",
        "        proj = getattr(self, f'hyper_proj_{name}')\n",
        "        scale = getattr(self, f'hyper_scale_{name}')\n",
        "        return scale(proj(hyper_h))\n",
        "\n",
        "    def forward(self, x, state, hyper_state, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Variable): A variable containing a float tensor\n",
        "                of size (batch_size, input_size).\n",
        "            state (tuple[Variable]): A tuple (h, c), each of which\n",
        "                is of size (batch_size, hidden_size).\n",
        "            hyper_state (tuple[Variable]): A tuple (hyper_h, hyper_c),\n",
        "                each of which is of size (batch_size, hyper_hidden_size).\n",
        "            mask (Variable): A variable containing a float tensor\n",
        "                of size (batch_size,).\n",
        "        Returns:\n",
        "            state (tuple[Variable]): The current state of the main LSTM.\n",
        "            hyper_state (tuple[Variable]): The current state of the\n",
        "                hyper LSTM.\n",
        "        \"\"\"\n",
        "\n",
        "        if state is None:\n",
        "            batch_size = x.size(0)\n",
        "            zero_state = Variable(\n",
        "                x.data.new(batch_size, self.hidden_size).zero_())\n",
        "            state = (zero_state, zero_state)\n",
        "\n",
        "        h, c = state\n",
        "\n",
        "        # Run a single step of Hyper LSTM.\n",
        "        hyper_input = torch.cat([x, h], dim=1)\n",
        "        new_hyper_h, new_hyper_state = self.hyper_cell(\n",
        "            x=hyper_input, state=hyper_state)\n",
        "\n",
        "        # Then, compute values for the main LSTM.\n",
        "        xh = self.linear_ih(x)\n",
        "        hh = self.linear_hh(h)\n",
        "\n",
        "        ix, fx, gx, ox = xh.chunk(chunks=4, dim=1)\n",
        "        ix = ix * self.compute_hyper_vector(hyper_h=new_hyper_h, name='ix')\n",
        "        fx = fx * self.compute_hyper_vector(hyper_h=new_hyper_h, name='fx')\n",
        "        gx = gx * self.compute_hyper_vector(hyper_h=new_hyper_h, name='gx')\n",
        "        ox = ox * self.compute_hyper_vector(hyper_h=new_hyper_h, name='ox')\n",
        "\n",
        "        ih, fh, gh, oh = hh.chunk(chunks=4, dim=1)\n",
        "        ih = ih * self.compute_hyper_vector(hyper_h=new_hyper_h, name='ih')\n",
        "        fh = fh * self.compute_hyper_vector(hyper_h=new_hyper_h, name='fh')\n",
        "        gh = gh * self.compute_hyper_vector(hyper_h=new_hyper_h, name='gh')\n",
        "        oh = oh * self.compute_hyper_vector(hyper_h=new_hyper_h, name='oh')\n",
        "\n",
        "        ib, fb, gb, ob = self.bias.chunk(chunks=4, dim=0)\n",
        "        ib = ib + self.compute_hyper_vector(hyper_h=new_hyper_h, name='ib')\n",
        "        fb = fb + self.compute_hyper_vector(hyper_h=new_hyper_h, name='fb')\n",
        "        gb = gb + self.compute_hyper_vector(hyper_h=new_hyper_h, name='gb')\n",
        "        ob = ob + self.compute_hyper_vector(hyper_h=new_hyper_h, name='ob')\n",
        "\n",
        "        i = ix + ih + ib\n",
        "        f = fx + fh + fb + 1  # Set the initial forget bias to 1.\n",
        "        g = gx + gh + gb\n",
        "        o = ox + oh + ob\n",
        "\n",
        "        if self.use_layer_norm:\n",
        "            i, f, g, o = self.ln_ifgo(i, f, g, o)\n",
        "        new_c = c*f.sigmoid() + self.dropout(g.tanh())*i.sigmoid()\n",
        "        if self.use_layer_norm:\n",
        "            new_c = self.ln_c(new_c)\n",
        "        new_h = new_c.tanh() * o.sigmoid()\n",
        "\n",
        "        # Apply the mask vector.\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "            new_h = new_h*mask + h*(1 - mask)\n",
        "            new_c = new_c*mask + c*(1 - mask)\n",
        "\n",
        "        new_state = (new_h, new_c)\n",
        "        return new_h, new_state, new_hyper_state\n",
        "\n",
        "h = HyperLSTMCell()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlPaUPlPu7nW"
      },
      "source": [
        "# Coding playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwchNGeXaMeJ"
      },
      "source": [
        "Random = torch.randn(20,5,10,10)\n",
        "m=nn.LayerNorm(Random.size()[1:])\n",
        "m(Random)\n",
        "#Layer normalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuz7wnBI0HvG"
      },
      "source": [
        "Func=nn.KLDivLoss()\n",
        "k=torch.randn(3,3)\n",
        "l=torch.randn(3,3)\n",
        "print(Func(k,torch.exp(k))) #Why is the KLloss non-zero for identical outcomes?\n",
        "#It's because it's treating the second input as a log probability. Thus, we need to \n",
        "#exponentiate it such that the input for k and k to be the inputs, otherwise if we just type k,k it will be treated as k,log(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7l_Eq4xo2Wo"
      },
      "source": [
        "torch.nn.functional.kl_div(k,k) #Why isn't this zero, shouldn't distributions be identical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHvZ9skZMio1"
      },
      "source": [
        "batches = list(torch.utils.data.DataLoader(train_set, batch_size=1000, shuffle = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAb824KyMksx"
      },
      "source": [
        "batch = batches[0][0].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6m_8H-fMnfL"
      },
      "source": [
        "import PIL\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2egXoTgMote"
      },
      "source": [
        "batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vl0g7aSM4PS",
        "outputId": "8ad24676-4502-41c3-81fa-b49a01d14b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(batch[:, 0], batch[:, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTA9kM5yNJcH",
        "outputId": "80767c06-e902-41b6-90dd-81f1fe7d0b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# James can you fix this pls \n",
        "#Yes I can\n",
        "\n",
        "def sketchplotter(M):\n",
        "  X=[]\n",
        "  Y=[]\n",
        "  #these 'all' variables store every jump in its entirety, the X and Y omit those jumps which are the pen being lifted.\n",
        "  XALL=[]\n",
        "  YALL=[]\n",
        "  strokenumber=0\n",
        "  for i in range(len(M)): #Originally taking tensor data with size(), had to change to len since it is a numpy tuple\n",
        "    if i == 0:\n",
        "      X.append(M[i,0])\n",
        "      Y.append(M[i,1])\n",
        "      XALL.append(M[i,0])\n",
        "      YALL.append(M[i,1])\n",
        "    elif M[i,3].item()==1: #The slot for the lift up state changed from 2-> 3 when we reformatted the data\n",
        "      X.append(M[i,0]+XALL[i-1])\n",
        "      Y.append(M[i,1]+YALL[i-1])\n",
        "      XALL.append(M[i,0]+XALL[i-1])\n",
        "      YALL.append(M[i,1]+YALL[i-1])\n",
        "      strokenumber+=1\n",
        "      plt.plot(X,Y)\n",
        "      X=[]\n",
        "      Y=[]\n",
        "    else:\n",
        "      X.append(M[i,0]+XALL[i-1])\n",
        "      Y.append(M[i,1]+YALL[i-1])\n",
        "      XALL.append(M[i,0]+XALL[i-1])\n",
        "      YALL.append(M[i,1]+YALL[i-1])\n",
        "  plt.plot(X,Y)\n",
        "  print(strokenumber)\n",
        "  #print(X)\n",
        "  #print(Y)\n",
        "\n",
        "sketchplotter(batch)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}