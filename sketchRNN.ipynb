{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sketchRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ij264/Corpus-Drawing-Project/blob/master/sketchRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AhiOqKusgTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVUsFa3YsqFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhFbIJTVumIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "# UPDATE\n",
        "\n",
        "hp = {\n",
        "    'location': '/content/drive/Shared drives/Corpus Drawing Project/data/sketchrnn_airplane.npz',\n",
        "    'Nz': 128,\n",
        "    'batch_size': 100,\n",
        "    'encoder_hidden_size': 256,\n",
        "    'decoder_hidden_size': 512,\n",
        "    'temperature': 0.9,\n",
        "    'gradient_clipping': 1.0,\n",
        "    'lr': 1e-4,\n",
        "    'KL_min': 0.2,\n",
        "    'R': 0.9999,\n",
        "    'M': 20,\n",
        "    'WKL': 1.0,\n",
        "    'dropout': 0.1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g13oEwU71yxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0EV47XQsEl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DONE\n",
        "# returns maximum sequence length in stroke sequences in data\n",
        "def get_max_length(data):\n",
        "    sequences = [len(seq) for seq in data]\n",
        "    return max(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEEVArRzF92Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader(object):\n",
        "    def __init__(self,\n",
        "                 strokes,\n",
        "                 batch_size=5,\n",
        "                 random_scale_factor=0.0,\n",
        "                 augment_stroke_prob=0.0,\n",
        "                 limit=1000):\n",
        "        self.batch_size = batch_size # Batch size.\n",
        "        self.max_seq_length = get_max_length(strokes) # Nmax.\n",
        "        self.random_scale_factor = random_scale_factor # Data augmentation method.\n",
        "\n",
        "        # Removes large gaps in data. x and y offets are clamped to have absolute values no greater than this limit.\n",
        "        self.limit = limit\n",
        "        self.augment_stroke_prob = augment_stroke_prob\n",
        "        self.start_stroke_token = torch.Tensor([0, 0, 1, 0, 0]) # S_0 in the paper.\n",
        "\n",
        "        # sets self.strokes: list of arrays (sorted by size), one per sketch, in stroke-3 format (DeltaX, DeltaY, pen binary state)\n",
        "        self.preprocess(strokes)\n",
        "        self.pad_data(self.strokes, self.max_seq_length)\n",
        "        self.normalise()\n",
        "    \n",
        "    def preprocess(self, strokes): \n",
        "        # Removes entries from strokes having a sequence longer than max_seq_lengths\n",
        "        raw_data = []\n",
        "        seq_len = []\n",
        "        count_data = 0\n",
        "        \n",
        "        for data in strokes:\n",
        "\n",
        "            if len(data) <= (self.max_seq_length):\n",
        "                count_data += 1\n",
        "                # removes large gaps from the data\n",
        "                data = np.minimum(data, self.limit)\n",
        "                data = np.maximum(data, -self.limit)\n",
        "                raw_data.append(data)\n",
        "                seq_len.append(len(data))\n",
        "\n",
        "        seq_len = np.array(seq_len)  # n strokes for each sketch\n",
        "        idx = np.argsort(seq_len)\n",
        "\n",
        "        self.strokes = []\n",
        "\n",
        "        for i in range(len(seq_len)):\n",
        "            self.strokes.append(raw_data[idx[i]])\n",
        "\n",
        "        print(\"total images <= max_seq_len is %d\" % count_data)\n",
        "\n",
        "        self.num_batches = int(count_data / self.batch_size)\n",
        "        return self.strokes\n",
        "\n",
        "    def calculate_normalizing_scale_factor(self):\n",
        "        \"\"\"Calculate the normalizing factor explained in appendix of sketch-rnn.\"\"\"\n",
        "        return torch.std(self.strokes) \n",
        "\n",
        "    def normalise(self):\n",
        "        ''' Normalise entire dataset by normalising factor '''\n",
        "        scale_factor = self.calculate_normalizing_scale_factor()\n",
        "        self.strokes[:,:,0:2] /= scale_factor\n",
        "        return self.strokes\n",
        "\n",
        "    def pad_data(self, data, max_len):\n",
        "        ''' Pad the batch to be stroke-5 bigger format as described in paper. '''\n",
        "        padded_data = np.zeros((len(data), max_len + 1, 5), dtype=float)\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            l = len(data[i])\n",
        "            assert l <= max_len\n",
        "            padded_data[i, 0:l, 0:2] = data[i][:, 0:2]\n",
        "            padded_data[i, 0:l, 3] = data[i][:, 2]\n",
        "            padded_data[i, 0:l, 2] = 1 - padded_data[i, 0:l, 3]\n",
        "            padded_data[i, l:, 4] = 1\n",
        "            # put in the first token, as described in sketch-rnn methodology\n",
        "            padded_data[i, 1:, :] = padded_data[i, :-1, :]\n",
        "            padded_data[i, 0, :] = 0\n",
        "            padded_data[i, 0, 2] = self.start_stroke_token[2]  # setting S_0 from paper.\n",
        "            padded_data[i, 0, 3] = self.start_stroke_token[3]\n",
        "            padded_data[i, 0, 4] = self.start_stroke_token[4]\n",
        "\n",
        "        self.strokes = torch.from_numpy(padded_data)\n",
        "        return self.strokes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlRExiGalFvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.load(hp['location'], encoding='latin1', allow_pickle=True)\n",
        "train_strokes = data['train']\n",
        "test_strokes = data['test']\n",
        "train_set = DataLoader(train_strokes).strokes\n",
        "\n",
        "batches = list(torch.utils.data.DataLoader(train_set, batch_size=1000, shuffle = True)) # this function creates batches automatically\n",
        "# preprocessing done!!!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64eIhgf_tGXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UPDATE\n",
        "# encoder RNN\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # bidirectional LSTM \n",
        "        self.LSTM = nn.LSTM(5, # input vector is 5x1\n",
        "                            hp['encoder_hidden_size'],\n",
        "                            bidirectional=True\n",
        "                            )\n",
        "        self.dropout = nn.Dropout(hp['dropout'])\n",
        "        # mu and sigma from LSTM's output\n",
        "        self.fc_mu = nn.Linear(2*hp['encoder_hidden_size'],\n",
        "                               hp['Nz'])\n",
        "        self.fc_sigma = nn.Linear(2*hp['encoder_hidden_size'],\n",
        "                                  hp['Nz'])\n",
        "        \n",
        "        self.train()\n",
        "\n",
        "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
        "        if hidden_cell == None:\n",
        "            # initialise with zeros\n",
        "            hidden = torch.zeros(2, batch_size, hp['encoder_hidden_size'])\n",
        "            cell = torch.zeros(2, batch_size, hp['encoder_hidden_size'])\n",
        "            hidden_cell = (hidden, cell)\n",
        "\n",
        "        _, (hidden, cell) = self.LSTM(inputs.float(), hidden_cell) # returns hidden state and cell vector. we discard the output tensor\n",
        "        hidden_forward, hidden_backward = torch.split(self.dropout(hidden), 1, 0) # returns forward and backwards\n",
        "        hidden_concat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)], 1) # concatenates the forwad and backwards h \n",
        "\n",
        "        mu = self.fc_mu(hidden_concat)\n",
        "        sigma_hat = self.fc_sigma(hidden_concat)\n",
        "        sigma_hat = self.sigma(h)\n",
        "        sigma = torch.exp(sigma_hat/2)\n",
        "\n",
        "        N = torch.normal(torch.zeros(mu.size()), torch.ones(mu_size())) \n",
        "\n",
        "        z = mu + N * sigma\n",
        "        \n",
        "        return z, mu, sigma_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUREW3cW9xsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.fc_hc = nn.Linear(2 * hp['Nz'],\n",
        "                               hp['decoder_hidden_size'])\n",
        "        \n",
        "        self.LSTM = nn.LSTM(hp['Nz'] + 5, # input of decoder is output of encoder (latent vector of size Nz) as well as the previous data point, S_{i-1}\n",
        "                            hp['decoder_hidden_size'])\n",
        "        self.dropout = nn.Dropout(hp['dropout'])\n",
        "        # output vector, y \n",
        "        y = nn.Linear() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFCUhifSszKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NN model: a bidirectional NN with LSTM\n",
        "class Model():\n",
        "    def __init__(self):\n",
        "\n",
        "        # forward encoder\n",
        "        self.encoder = EncoderRNN()\n",
        "\n",
        "        # backward encoder\n",
        "        self.decoder = DecoderRNN()\n",
        "\n",
        "        # TODO: implement gradient clipping\n",
        "        self.encoder_optimiser = optim.Adam(self.encoder.parameters(), hp['lr'])\n",
        "        self.decoder_optimiser = optim.Adam(self.decoder.parameters(), hp['lr'])\n",
        "\n",
        "    def something\n",
        "    # bivariate normal distribution probability distribution function\n",
        "    '''def bivariate_normal_PDF(Dx, Dy):\n",
        "\n",
        "        z = (Dx - self.mu_x)**2/self.sigma_x**2 \\\n",
        "        - 2 * self.rho_xy * (Dx - self.mu_x) * (Dy - self.mu_y)/(self.sigma_x * self.sigma_y) \\\n",
        "        + (Dy - self.mu_y)**2/self.sigma_y**2\n",
        "        prefactor = 1/(2 * np.pi * self.sigma_x * self.sigma_y * torch.sqrt(1 - self.rho_xy**2))\n",
        "\n",
        "        return prefactor * torch.exp(-z/(2 * (1 - self.rho_xy**2)))\n",
        "       '''\n",
        "       #What we want is to be able to have a given Dx and Dy offset and then compute the probabilities of having these offsets given a mean and a standard deviation. \n",
        "        mu_x=0\n",
        "        mu_y=0\n",
        "        sigma_x=1\n",
        "        sigma_y=1\n",
        "        M=torch.distributions.multivariate_normal.MultivariateNormal(torch.tensor([mu_x,mu_y]).float(), \\\n",
        "         covariance_matrix=torch.tensor(([sigma_x,0],[0,sigma_y])).float())  \n",
        "\n",
        "\n",
        "    # reconstruction loss\n",
        "    def LR(self, Dx, Dy, p):\n",
        "        #PDF = bivariate_normal_PDF(Dx, Dy) \n",
        "        PDF = torch.exp(M.log_prob(torch.tensor([Dx,Dy]))).item() #If you have any problems, might be the .item() on the end of this line.\n",
        "        LS = -1/float(N_max) * torch.sum(\n",
        "            torch.log(\n",
        "                torch.sum(self.Pi * PDF)\n",
        "            )\n",
        "        )\n",
        "        LP = -1/float(N_max) * torch.sum(\n",
        "            p * torch.log(self.q)  #Do we need to sum over this twice for our two indicies, p and q\n",
        "        )\n",
        "        return LS + LP\n",
        "\n",
        "    # KL divergence loss \n",
        "    # use pytorch function for this (James?)\n",
        "    def KL(self, Dx, Dy):\n",
        "        return -1/(2 * float(hp['Nz'])) * torch.sum(1 + sigma_hat - torch.square(mu)-torch.exp(sigma_hat))\n",
        "\n",
        "# EQ. 7 USE SOFTMAX IN PYTORCH\n",
        "    #q = nn.Softmax(self.q_hat) \n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "\n",
        "        batch, lengths = get_batch(hp['batch_size'])\n",
        "        \n",
        "        z, self.mu, self.sigma_hat = self.encoder(batch, hp['batch_size'])\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem \n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "\n",
        "        S_0 = torch.stack([torch.Tensor([0, 0, 1, 0, 0])] * hp['batch_size']).unsqueeze(0)\n",
        "        initial_batch = torch.cat([S_0, batch], 0)\n",
        "        stacked_z = torch.stack([z] * (Nmax + 1))\n",
        "        inputs = torch.cat([inital_batch, z_stack], 2)\n",
        "        \n",
        "        # at each step in time, the current data point as well as z is inputted into the decoder.\n",
        "        self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, self.rho_xy, self.q, _, _ = self.decoder(inputs, z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlPaUPlPu7nW",
        "colab_type": "text"
      },
      "source": [
        "# Coding playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwchNGeXaMeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Random = torch.randn(20,5,10,10)\n",
        "m=nn.LayerNorm(Random.size()[1:])\n",
        "m(Random)\n",
        "#Layer normalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuz7wnBI0HvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Func=nn.KLDivLoss()\n",
        "k=torch.randn(3,3)\n",
        "l=torch.randn(3,3)\n",
        "print(Func(k,k)) #Why is the KLloss non-zero for identical outcomes?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7l_Eq4xo2Wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.nn.functional.kl_div(k,k) #Why isn't this zero, shouldn't distributions be identical"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}