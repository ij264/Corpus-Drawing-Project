{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sketchRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ij264/Corpus-Drawing-Project/blob/master/sketchRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AhiOqKusgTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVUsFa3YsqFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhFbIJTVumIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "# UPDATE\n",
        "\n",
        "hp = {\n",
        "    'location': '/content/drive/Shared drives/Corpus Drawing Project/data/sketchrnn_airplane.npz',\n",
        "    'Nz': 128,\n",
        "    'batch_size': 1,\n",
        "    'encoder_hidden_size': 256,\n",
        "    'decoder_hidden_size': 512,\n",
        "    'temperature': 0.9,\n",
        "    'gradient_clipping': 1.0,\n",
        "    'lr': 1e-4,\n",
        "    'KL_min': 0.2,\n",
        "    'R': 0.9999,\n",
        "    'M': 20,\n",
        "    'WKL': 1.0,\n",
        "    'dropout': 0.1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g13oEwU71yxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzau4wdQesAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DONE\n",
        "# returns maximum sequence length in stroke sequences in data\n",
        "def max_size(data):\n",
        "    sequences = [len(seq) for seq in data]\n",
        "    return max(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upQEhcxKyZUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.load(hp['location'], encoding='latin1', allow_pickle=True)\n",
        "training_data = data['train']\n",
        "testing_data = data['test']\n",
        "\n",
        "Nmax = max_size(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KgXifpFa8IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns batches of size batch_size\n",
        "def get_batch(data, batch_size):\n",
        "    batch_idx = np.random.choice(len(data), batch_size) # creates array of random indices of length batch_size\n",
        "    batch_sequence = [data[idx] for idx in batch_idx]\n",
        "    strokes = []\n",
        "    lengths = []\n",
        "\n",
        "    for sequence in batch_sequence:\n",
        "        sequence_len = len(sequence[:, 0]) # length of first column\n",
        "        new_sequence = np.zeros((Nmax, 5)) # initalises empty sequence to store strokes. each row is in the form DeltaX, DeltaY, p1, p2, p3\n",
        "        new_sequence[:sequence_len, :2] = sequence[:, :2] # initalises DeltaX, DeltaY\n",
        "        new_sequence[:sequence_len - 1, 2] = 1-sequence[:-1, 2] # initalises p1\n",
        "        new_sequence[:sequence_len, 3] = sequence[:, 2] # initialises p2\n",
        "        new_sequence[(sequence_len - 1):, 4] = 1 # initialises p3\n",
        "        new_sequence[sequence_len - 1, 2:4] = 0\n",
        "        lengths.append(len(sequence[:, 0]))\n",
        "        strokes.append(new_sequence)\n",
        "    \n",
        "    batch = Variable(torch.from_numpy(np.stack(strokes, 1)).float())\n",
        "    \n",
        "    return batch, lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64eIhgf_tGXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UPDATE\n",
        "# encoder RNN\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # bidirectional LSTM \n",
        "        self.LSTM = nn.LSTM(input_size=5, # input vector is 5x1\n",
        "                            hp['encoder_hidden_size'],\n",
        "                            hp['decoder_hidden_size'],\n",
        "                            num_layers=1,\n",
        "                            bias=True,\n",
        "                            batch_first=False,\n",
        "                            bidirectional=True\n",
        "                            )\n",
        "        self.dropout = nn.Dropout(hp['dropout'])\n",
        "        # mu and sigma from LSTM's output\n",
        "        self.fc_mu = nn.Linear(in_features=2*hp['encoder_hidden_size'],\n",
        "                            hp.Nz)\n",
        "        self.fc_sigma = nn.Linear(in_features=2*hp['encoder_hidden_size'],\n",
        "                               hp['Nz'])\n",
        "        \n",
        "        self.train()\n",
        "\n",
        "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
        "        if hidden_cell == None:\n",
        "            # initialise with zeros\n",
        "            hidden = torch.zeros(2, batch_size, hp['encoder_hidden_size'])\n",
        "            cell = torch.zeros(2, batch_size, hp['encoder_hidden_size'])\n",
        "            hidden_cell = (hidden, cell)\n",
        "\n",
        "        _, (hidden, cell) = self.LSTM(inputs.float(), hidden_cell) # returns hidden state and cell vector. we discard the output tensor\n",
        "        hidden_forward, hidden_backward = torch.split(self.dropout(hidden), 1, 0) # returns forward and backwards\n",
        "        hidden_concat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)], 1) # concatenates the forwad and backwards h \n",
        "\n",
        "        mu = self.fc_mu(hidden_concat)\n",
        "        sigma_hat = self.fc_sigma(hidden_concat)\n",
        "        sigma_hat = self.sigma(h)\n",
        "        sigma = torch.exp(sigma_hat/2)\n",
        "\n",
        "        N = torch.normal(torch.zeros(mu.size()), torch.ones(mu_size()))\n",
        "\n",
        "        z = mu + N * sigma\n",
        "        \n",
        "        return z, mu, sigma_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUREW3cW9xsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    # stuff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFCUhifSszKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NN model: a bidirectional NN with LSTM\n",
        "class Model():\n",
        "    def __init__(self):\n",
        "\n",
        "        # forward encoder\n",
        "        self.encoder() = EncoderRNN()\n",
        "\n",
        "        # backward encoder\n",
        "        self.decoder() = DecoderRNN()\n",
        "\n",
        "        # TODO: implement gradient clipping\n",
        "        self.encoder_optimiser() = optim.Adam(self.encoder.parameters(), hp['lr'])\n",
        "        self.decoder_optimiser() = optim.Adam(self.decoder.parameters(), hp['lr'])\n",
        "\n",
        "    # bivariate normal distribution probability distribution function\n",
        "    def bivariate_normal_PDF(Dx, Dy):\n",
        "\n",
        "        z = (Dx - self.mu_x)**2/self.sigma_x**2 \\\n",
        "        - 2 * self.rho_xy * (Dx - self.mu_x) * (Dy - self.mu_y)/(self.sigma_x * self.sigma_y) \\\n",
        "        + (Dy - self.mu_y)**2/self.sigma_y**2\n",
        "        prefactor = 1/(2 * np.pi * self.sigma_x * self.sigma_y * torch.sqrt(1 - self.rho_xy**2))\n",
        "\n",
        "        return prefactor * torch.exp(-z/(2 * (1 - self.rho_xy**2)))\n",
        "       \n",
        "       #This is using the inbuilt function from Pytorch, if you want to generate a random vector from it you use M.sample() and it'll create a vector with two elements. \n",
        "        '''\n",
        "        mx=0\n",
        "        my=0\n",
        "        sx=1\n",
        "        sy=1\n",
        "        M=torch.distributions.multivariate_normal.MultivariateNormal(torch.tensor([mx,my]).float(), \\\n",
        "         covariance_matrix=torch.tensor(([sx,0],[0,sy])).float())\n",
        "        '''\n",
        "\n",
        "    # reconstruction loss\n",
        "    def LR(self, Dx, Dy, p):\n",
        "        PDF = bivariate_normal_PDF(Dx, Dy)\n",
        "        LS = -1/float(N_max) * torch.sum(\n",
        "            torch.log(\n",
        "                torch.sum(self.Pi * PDF)\n",
        "            )\n",
        "        )\n",
        "        LP = -1/float(N_max) * torch.sum(\n",
        "            p * torch.log(self.q)\n",
        "        )\n",
        "        return LS + LP\n",
        "\n",
        "    # KL divergence loss \n",
        "    def KL(self, Dx, Dy):\n",
        "        return -1/(2 * float(hp['Nz'])) * torch.sum(1 + sigma_hat - torch.square(mu))\n",
        "\n",
        "# EQ. 7 USE SOFTMAX IN PYTORCH\n",
        "    q = nn.Softmax(self.q_hat) \n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "\n",
        "        batch, lengths = get_batch(hp['batch_size'])\n",
        "        \n",
        "        total_loss = 0\n",
        "        for i, (data, _) in enumerate(training_set):\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}